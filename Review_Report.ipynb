{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Review Report.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/z1065384049/UTS_ML2019_ID12782886/blob/master/Review_Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s3me7xQvFta",
        "colab_type": "text"
      },
      "source": [
        "# **Review Report on \"Support-Vector Networks\"**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtND3S4bvdi7",
        "colab_type": "text"
      },
      "source": [
        "12782886 Zhiliang Zhao"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f3avpHCwC9w",
        "colab_type": "text"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EZzQxADwc7b",
        "colab_type": "text"
      },
      "source": [
        "This paper focuses on reviewing an article named ‘Support-Vector Networks’. the main focus of the review is to determined the technical problem associated with Support-Vector Networks and the testing of this classifier in reality. Moreover, the three ideas presented in the article and the comparison with other classifiers are also taken into consideration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4I3UcGKywrgR",
        "colab_type": "text"
      },
      "source": [
        "## Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZIhX1Mlw31P",
        "colab_type": "text"
      },
      "source": [
        "The research is about a new learning machine for two-group classification problems. The support vector machine is the decision surface(or hyperplane) used to separate the data points. Its position is determined by the support vector (if the support vector changes, the position of the separation surface will also change). Therefore, the surface is a classifier which determined by support vector, it called support vector machine. Support vector network is an improved maximal margin method that can handle mislabeled samples, proposed by Corinna Cortes and Vapnik in 1995. In the article, SVM is divided into three categories: Optimal Hyperplanes , The Soft Margin Hyperplane, and The Method of Convolution of the Dot-Product in Feature. Space.\n",
        "\n",
        "The hard-margin classification is a linear separable support vector machine. Assuming that the straight line is y=wx+b, then as long as the sum of all positive classification points to the line and the sum of all negative classification points to the line is maximized, this line is the optimal classification line. In this way, the original problem is transformed into a constrained optimization problem, which can be solved directly. This algorithm is called The Optimal Hyperplane Algorithm.\n",
        "\n",
        "The soft-margin classification is a linear non-separable support vector machine. The reason for use soft-margin is that there are some training set samples (noise point) do not satisfy the condition(function interval is greater than or equal to 1). So that add a non-negative parameter C (slack variable), and let the function interval plus C satisfy the condition. By adding a slack variable, the error caused by the new slack variable needs to be added to the original distance function, so that the final optimization function becomes two parts: the distance function and the slack variable error. The model can be controlled by adjusting the parameter C. The algorithm used is called The Soft Margin Hyperplane.\n",
        "\n",
        "The nonlinear support vector machine corresponds to ‘The Method of Convolution of the Dot-Product’. For data that is linearly inseparable in N-dimensional space, the space above N+1 is more likely to become linearly separable (not necessarily linearly separable in N+1 dimension). The higher the dimension, the more likely it is to be linearly separable.For linearly inseparable data, it can be mapped to a new linearly separable space, which can then be solved using the hard-margin support vector machine or soft-margin support vector machine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "051yr_1TxXTB",
        "colab_type": "text"
      },
      "source": [
        "## Innovation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mxnwMJe13Zv",
        "colab_type": "text"
      },
      "source": [
        "What the traditional SVM does is actually to find a hyperplane, to achieve two classifications, one class +1, one class -1. The support vector machine learning method for linear separable problems is not suitable for linear inseparable training data. How to solve the linear inseparable problem? The creative idea is support vector network allow for error on the training set. Usually there are some noisy points in the training data. After these noisy points are removed, the remaining sample sets are linearly separable. Linear inseparable means that some sample points cannot satisfy the constraint that the function interval is greater than or equal to 1. In order to solve this problem, it is proposed to introduce a slack variable for each sample point{X(i),Y(i)},and add the function interval plus the slack variable(ξi≥0)to be greater than or equal to 1, so that the constraint becomes:\n",
        "y(i)(w*x(i)+b)≥1−ξi, ξi≥0,i=1,2,...m\n",
        "The function becomes(1/2)w^2+C*F∑_(i=1)^m ξi\n",
        "C>0 is called the penalty parameter, which is generally determined by the application problem. When the C value is large, the penalty for misclassification increases, vice versa.\n",
        "\n",
        "When the minimum value of the objective function is required, there are two meanings in the slack variable and the penalty parameter:\n",
        "*   make (1/2)w^2 as small as possible, that is, the interval is as large as possible\n",
        "*   the slack variable is as small as possible, that is, the number of misclassified points is as small as possible;\n",
        "\n",
        "This result led to the promotion of the term \"support vector machine\" (or \"SVM\"). This method introduces a slack parameter to measure the misclassification of the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QQfwdoj60Kd",
        "colab_type": "text"
      },
      "source": [
        "## Technical quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4imrKKS87x-_",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The technical development if of high quality. The authors conducted two sets of experiments to validate the support vector network. The first experiments constructed an artificial data set, and the experimental results verified the feasibility of the algorithm. In the second experiment, 7300 training samples and 2000 test samples were studied. Ten classifiers were constructed in the experiment. The joint error of the ten classifiers on the test set was 1.1%. By comparison with other classifiers, the results show that the support vector network has a high degree of precision.The authors also point out that by constructing dot-product functions, support vector network performance can be further improved. The disadvantage is that the SVM algorithm is difficult to implement for large-scale training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFMc_z3b70Wr",
        "colab_type": "text"
      },
      "source": [
        "## Application and X-factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqZoCDPk734P",
        "colab_type": "text"
      },
      "source": [
        "Support vector machine has many advantages in solving small sample, nonlinear and high-dimensional pattern recognition, and can be widely used in statistical classification and regression analysis. The most widely used SVM is in the field of pattern recognition, which has been successfully used in many pattern recognition problems, including computer vision, web or text automatic classification, face recognition and so on. In addition, I think SVM can be applied to build predictive models, system fault detection.\n",
        "\n",
        "I think the future research direction is how to train large-scale datasets, solve the contradiction between training set size and training speed, and solve the contradiction between the number of support vectors and the classification speed. Furthermore, the choice of the SVM kernel function affects the performance of the classifier. How to choose the kernel function and parameters based on the actual sample data, these problems are lack of guidance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rTtFoGl78B8",
        "colab_type": "text"
      },
      "source": [
        "## Presentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv-uRCYP8EBT",
        "colab_type": "text"
      },
      "source": [
        "The article is a good illustration of the ideas and characteristics of support vector networks.It’s a bit difficult to understand all the symbols and formulas.The figures in this article have helped me a lot, making it easy to understand the concepts of neural networks and hyperplanes. If the author has more introductions to the kernel function in the article, not just direct references, I think there will be a better understanding of the nonlinear separability problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqpQ13bD8HNq",
        "colab_type": "text"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tudiuuYH8NOu",
        "colab_type": "text"
      },
      "source": [
        "Drucker, H., Burges, C.J., Kaufman, L., Smola, A.J. and Vapnik, V., 1997. Support vector regression machines. In Advances in neural information processing systems (pp. 155-161).\n",
        "\n",
        "Joachims, T., 2002. Learning to classify text using support vector machines (Vol. 668). Springer Science & Business Media.\n",
        "\n",
        "Suykens, J.A. and Vandewalle, J., 1999. Least squares support vector machine classifiers. Neural processing letters, 9(3), pp.293-300.\n"
      ]
    }
  ]
}